{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.autograd as torch_ad\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from firedrake import *\n",
    "from firedrake_adjoint import *\n",
    "from firedrake.ml.pytorch import torch_operator\n",
    "\n",
    "from physics_driven_ml.dataset_processing import PDEDataset, BatchedElement\n",
    "from physics_driven_ml.models import EncoderDecoder, CNN\n",
    "from physics_driven_ml.utils import ModelConfig, get_logger\n",
    "from physics_driven_ml.evaluation import evaluate\n",
    "\n",
    "\n",
    "\n",
    "def train(model, config: ModelConfig,\n",
    "          train_dl: DataLoader, dev_dl: DataLoader,\n",
    "          G: torch_ad.Function, H: torch_ad.Function):\n",
    "    \"\"\"Train the model on a given dataset.\"\"\"\n",
    "\n",
    "    optimiser = optim.AdamW(model.parameters(), lr=config.learning_rate, eps=1e-8)\n",
    "\n",
    "    max_grad_norm = 1.0\n",
    "    best_error = 0.\n",
    "\n",
    "    # Training loop\n",
    "    for epoch_num in trange(config.epochs):\n",
    "        logger.info(f\"Epoch num: {epoch_num}\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        total_loss_uk = 0.0\n",
    "        total_loss_k = 0.0\n",
    "        train_steps = len(train_dl)\n",
    "        for step_num, batch in tqdm(enumerate(train_dl), total=train_steps):\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Move batch to device\n",
    "            batch = BatchedElement(*[x.to(config.device, non_blocking=True) if isinstance(x, torch.Tensor) else x for x in batch])\n",
    "            k_exact = batch.target\n",
    "            u_obs = batch.u_obs\n",
    "\n",
    "            # Forward pass\n",
    "            k = model(u_obs)\n",
    "\n",
    "            # Solve PDE for κ_P and assemble the L2-loss: 0.5 * ||u(κ) - u_obs||^{2}_{L2}\n",
    "            loss_uk = G(k, u_obs)\n",
    "            total_loss_uk += loss_uk.item()\n",
    "\n",
    "            # Assemble L2-loss: 0.5 * ||κ - κ_exact||^{2}_{L2}\n",
    "            loss_k = H(k, k_exact)\n",
    "            total_loss_k += loss_k.item()\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_k + config.alpha * loss_uk\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Backprop and perform Adam optimisation\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "            optimiser.step()\n",
    "\n",
    "        logger.info(f\"Total loss: {total_loss/train_steps}\\\n",
    "                    \\n\\t Loss u(κ): {total_loss_uk/train_steps}  Loss κ: {total_loss_k/train_steps}\")\n",
    "\n",
    "        # Evaluation on dev set\n",
    "        error = evaluate(model, config, dev_dl, disable_tqdm=True)\n",
    "        logger.info(f\"Error ({config.evaluation_metric}): {error}\")\n",
    "\n",
    "        # Save best-performing model\n",
    "        if error < best_error or epoch_num == 0:\n",
    "            best_error = error\n",
    "            # Create directory for trained models\n",
    "            name_dir = f\"{config.dataset}-epoch-{epoch_num}-error_{best_error:.5f}\"\n",
    "            model_dir = os.path.join(config.data_dir, \"saved_models\", config.model_dir, name_dir)\n",
    "            if not os.path.exists(model_dir):\n",
    "                os.makedirs(model_dir)\n",
    "            # Save model\n",
    "            logger.info(f\"Saving model checkpoint to {model_dir}\\n\")\n",
    "            # Take care of distributed/parallel training\n",
    "            model_to_save = (model.module if hasattr(model, \"module\") else model)\n",
    "            torch.save(model_to_save.state_dict(), os.path.join(model_dir, \"model.pt\"))\n",
    "            # Save training arguments together with the trained model\n",
    "            config.to_file(os.path.join(model_dir, \"training_args.json\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger = get_logger(\"Training\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data_dir\", default=os.environ[\"DATA_DIR\"], type=str, help=\"Data directory\")\n",
    "    parser.add_argument(\"--model\", default=\"cnn\", type=str, help=\"one of [encoder-decoder, cnn]\")\n",
    "    parser.add_argument(\"--alpha\", default=1e4, type=float, help=\"Regularisation parameter\")\n",
    "    parser.add_argument(\"--epochs\", default=50, type=int, help=\"Epochs\")\n",
    "    parser.add_argument(\"--batch_size\", default=1, type=int, help=\"Batch size\")\n",
    "    parser.add_argument(\"--learning_rate\", default=5e-5, type=float, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--dropout\", default=0.1, type=float, help=\"Dropout rate\")\n",
    "    parser.add_argument(\"--evaluation_metric\", default=\"L2\", type=str, help=\"Evaluation metric: one of [Lp, H1, Hdiv, Hcurl, , avg_rel]\")\n",
    "    parser.add_argument(\"--max_eval_steps\", default=5000, type=int, help=\"Maximum number of evaluation steps\")\n",
    "    parser.add_argument(\"--dataset\", default=\"heat_conductivity\", type=str, help=\"Dataset name\")\n",
    "    parser.add_argument(\"--model_dir\", default=\"model\", type=str, help=\"Directory name to save trained models\")\n",
    "    parser.add_argument(\"--device\", default=\"cpu\", type=str, help=\"Device identifier (e.g. 'cuda:0' or 'cpu')\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    config = ModelConfig(**dict(args._get_kwargs()))\n",
    "\n",
    "    # -- Load dataset -- #\n",
    "\n",
    "    # Load train dataset\n",
    "    train_dataset = PDEDataset(dataset=config.dataset, dataset_split=\"train\", data_dir=config.data_dir)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, collate_fn=train_dataset.collate, shuffle=False)\n",
    "    # Load test dataset\n",
    "    test_dataset = PDEDataset(dataset=config.dataset, dataset_split=\"test\", data_dir=config.data_dir)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, collate_fn=test_dataset.collate, shuffle=False)\n",
    "\n",
    "    # -- Set PDE inputs (mesh, function space, boundary conditions, ...) -- #\n",
    "\n",
    "    # Get mesh from dataset\n",
    "    mesh = train_dataset.mesh\n",
    "    # Define function space and test function\n",
    "    V = FunctionSpace(mesh, \"CG\", 1)\n",
    "    v = TestFunction(V)\n",
    "    # Define right-hand side\n",
    "    x, y = SpatialCoordinate(mesh)\n",
    "    with stop_annotating():\n",
    "        f = Function(V).interpolate(sin(pi * x) * sin(pi * y))\n",
    "    # Define Dirichlet boundary conditions\n",
    "    bcs = [DirichletBC(V, Constant(0.0), \"on_boundary\")]\n",
    "\n",
    "    # -- Define the Firedrake operations to be composed with PyTorch -- #\n",
    "\n",
    "    def solve_pde(k, u_obs, f, V, bcs):\n",
    "        \"\"\"Solve Poisson problem\"\"\"\n",
    "        # u = Function(V)\n",
    "        # v = TestFunction(V)\n",
    "        # F = (inner(exp(k) * grad(u), grad(v)) - inner(f, v)) * dx\n",
    "        # # Solve PDE (using LU factorisation)\n",
    "        # solve(F == 0, u, bcs=bcs, solver_parameters={'ksp_type': 'preonly', 'pc_type': 'lu'})\n",
    "        # # Assemble Firedrake L2-loss (and not l2-loss as in PyTorch)\n",
    "        return assemble_L2_error(u, u_obs)\n",
    "\n",
    "    def assemble_L2_error(x, x_exact):\n",
    "        \"\"\"Assemble L2-loss\"\"\"\n",
    "        return assemble(0.5 * (x - x_exact) ** 2 * dx)\n",
    "\n",
    "    solve_pde = functools.partial(solve_pde, f=f, V=V, bcs=bcs)\n",
    "\n",
    "    # -- Construct the Firedrake torch operators -- #\n",
    "\n",
    "    k = Function(V)\n",
    "    u_obs = Function(V)\n",
    "    k_exact = Function(V)\n",
    "\n",
    "    # Set tape locally to only record the operations relevant to G on the computational graph\n",
    "    with set_working_tape() as tape:\n",
    "        # Define PyTorch operator for solving the PDE and compute the L2 error (for computing κ -> 0.5 * ||u(κ) - u_obs||^{2}_{L2})\n",
    "        F = ReducedFunctional(solve_pde(k, u_obs), [Control(k), Control(u_obs)])\n",
    "        G = torch_operator(F)\n",
    "\n",
    "    # Set tape locally to only record the operations relevant to H on the computational graph\n",
    "    with set_working_tape() as tape:\n",
    "        # Define PyTorch operator for computing the L2-loss (for computing κ -> 0.5 * ||κ - κ_exact||^{2}_{L2})\n",
    "        F = ReducedFunctional(assemble_L2_error(k, k_exact), [Control(k), Control(k_exact)])\n",
    "        H = torch_operator(F)\n",
    "\n",
    "\n",
    "    model = NN()\n",
    "    # Set double precision (default Firedrake type)\n",
    "    model.double()\n",
    "    # Move model to device\n",
    "    model.to(config.device)\n",
    "\n",
    "\n",
    "    train(model, config=config, train_dl=train_dataloader, dev_dl=test_dataloader, G=G, H=H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firedrake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
